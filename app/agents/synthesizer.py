from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from app.state import AgentState
from app.models import FinalReport

class SynthesizerAgent:
    def __init__(self, model_name="mistral"):
        self.llm = ChatOllama(model=model_name, temperature=0)
        
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a professional research report writer. "
                       "Synthesize the provided findings into a comprehensive markdown report. "
                       "Include an Executive Summary, Detailed Findings (with tables if possible), and Sources."),
            ("user", "Research Query: {query}\n\nFindings:\n{findings}")
        ])
        
        self.chain = self.prompt | self.llm

    async def synthesize(self, state: AgentState) -> dict:
        print(f"--- SYNTHESIZER: Generating report for {len(state['findings'])} findings ---")
        
        findings_text = "\n\n".join([f"Source: {f.source_url}\nContent: {f.content}" for f in state['findings']])
        
        response = await self.chain.ainvoke({"query": state['query'], "findings": findings_text})
        
        report_content = response.content
        
        final_report = FinalReport(
            summary="Generated by TinyScout",
            findings=state['findings'],
            sources=[f.source_url for f in state['findings']],
            formatted_report=report_content
        )
        
        return {"final_report": final_report, "messages": ["Report synthesized."]}
